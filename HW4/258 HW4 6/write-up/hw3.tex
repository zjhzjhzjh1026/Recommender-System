\documentclass{assignment}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multirow}
\usepackage{pythonhighlight}
\lstset{
numbers=left
}

\coursetitle{Web Mining and Recommender Systems}
\courselabel{CSE 258}
\exercisesheet{Homework Four}{}
\student{Zhang, Jinhan PID A53211930}
\university{University of California, San Diego}
\semester{Winter 2017}
\date{March 6, 2017}

\begin{document}
\begin{problemlist}




\pbitem Solution

The number of unique bigrams: 182246
\begin{table}[h]
\vspace{2ex}
\centering
\caption{The 5 most-frequently-occurring bigrams:}
\vspace{1ex}

\begin{tabular}{|c|c|}
\hline
Bigrams & The number of occurrences \\
\hline
'with a' & 4587\\
\hline
'in the' & 2595 \\
\hline
'of the' & 2245 \\
\hline
'is a' & 2056 \\
\hline
'on the' & 2033 \\
\hline
\end{tabular}
\end{table}


\vspace{2ex}
\begin{center} 
Listing 1: Key code for Prob.1
\end{center}
\begin{python}
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  w = r.split()
  for i in range(len(w)-1):
    wordCount[w[i]+' '+w[i+1]] += 1
len(wordCount)

counts = [(wordCount[w] , w) for w in wordCount]
counts.sort()
counts.reverse()
counts
\end{python}

\vspace{3ex}

\pbitem Solution

MSE using bigrams: 0.34315301406136378


\begin{center} 
Listing 2: Key code for Prob.2
\end{center}
\begin{python}
words = [x[1] for x in counts[:1000]]
wordId = dict(zip(words, range(len(words))))
wordSet = set(words)

def feature(datum):
  feat = [0]*len(words)
  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])
  w = r.split()
  for i in range(len(w)-1):
    bitemp=w[i]+' '+w[i+1]
    if bitemp in words:
      feat[wordId[bitemp]] += 1
  feat.append(1) #offset
  return feat

X = [feature(d) for d in data]
y = [d['review/overall'] for d in data]

clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(X, y)
theta = clf.coef_
predictions = clf.predict(X)
diff = predictions-y
mse = sum([t**2 for t in diff])/len(diff)
mse
\end{python}

\pbitem Solution

MSE using unigrams and bigrams: 0.28904733303355806

\begin{center} 
Listing 3: Key code for Prob.3
\end{center}
\begin{python}
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  w = r.split()
  for i in range(len(w)-1):
    wordCount[w[i]+' '+w[i+1]] += 1
  for tmp in w:
    wordCount[tmp] += 1
len(wordCount)

counts = [(wordCount[w] , w) for w in wordCount]
counts.sort()
counts.reverse()
# counts
words = [x[1] for x in counts[:1000]]
wordId = dict(zip(words, range(len(words))))
wordSet = set(words)

def feature(datum):
  feat = [0]*len(words)
  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])
  w = r.split()
  for i in range(len(w)-1):
    bitemp=w[i]+' '+w[i+1]
    if bitemp in words:
      feat[wordId[bitemp]] += 1
  for tmp in w:
    if tmp in words:
      feat[wordId[tmp]] += 1
  feat.append(1) #offset
  return feat

X = [feature(d) for d in data]
y = [d['review/overall'] for d in data]

clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(X, y)
theta = clf.coef_
predictions = clf.predict(X)

diff = predictions-y
mse = sum([t**2 for t in diff])/len(diff)
\end{python}

\pbitem Solution

\begin{table}[h]
\vspace{2ex}
\centering
\caption{The 5 unigrams/bigrams with the most positive associated weights:}
\vspace{1ex}

\begin{tabular}{|c|c|}
\hline
Unigrams/Bigrams & Weights \\
\hline
'sort' & 0.51982780120456751\\
\hline
'a bad' & 0.22881971426910591 \\
\hline
'of these' & 0.22283470424121538 \\
\hline
'not bad' & 0.21687721630732146 \\
\hline
'the best' & 0.20639109567227393 \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\vspace{2ex}
\centering
\caption{The 5 unigrams/bigrams with the most negative associated weights:}
\vspace{1ex}

\begin{tabular}{|c|c|}
\hline
Unigrams/Bigrams & Weights \\
\hline
'sort of' & -0.63976214971855316\\
\hline
'water' & -0.27048649882966247 \\
\hline
'corn' & -0.23703101460442585 \\
\hline
'the background'& -0.21624829959516467 \\
\hline
'straw' & -0.19593772177944399 \\
\hline
\end{tabular}
\end{table}

\begin{center} 
Listing 4: Key code for Prob.4
\end{center}
\begin{python}
weight = [(theta[i] , words[i]) for i in range(len(words))]
weight.sort()
#weight

weight.reverse()
#weight
\end{python}

\pbitem Solution

\begin{table}[H]
\vspace{2ex}
\centering
\caption{Inverse document frequency:}
\vspace{1ex}

\begin{tabular}{|c|c|}
\hline
Words & Idf \\
\hline
'foam' & 1.1378686206869628\\
\hline
'smell' &  0.5379016188648442 \\
\hline
'banana' & 1.6777807052660807 \\
\hline
'lactic'& 2.9208187539523753 \\
\hline
'tart' & 1.8068754016455384 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\vspace{2ex}
\centering
\caption{Tf-idf Scores in the first review:}
\vspace{1ex}

\begin{tabular}{|c|c|}
\hline
Words & Tf-idf \\
\hline
'foam' & 2.2757372413739256\\
\hline
'smell' &  0.5379016188648442 \\
\hline
'banana' & 3.3555614105321614 \\
\hline
'lactic'& 5.841637507904751 \\
\hline
'tart' &  1.8068754016455384 \\
\hline
\end{tabular}
\end{table}

\begin{center} 
Listing 5: Key code for Prob.5
\end{center}
\begin{python}
wordList = ['foam','smell','banana','lactic','tart']
wordCount = defaultdict(int)
punctuation = set(string.punctuation)

for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  w = r.split()
  for word in wordList:
    if word in w:
      wordCount[word] += 1

freq = [math.log10(len(data) * 1.0 /wordCount[word]) for word in wordList]
#freq

d=data[0]
wordCount2 = defaultdict(int)
r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
w = r.split()
for word in w:
    if word in wordList:
      wordCount2[word] += 1

tfidf=[freq[i]*wordCount2[wordList[i]] for i in range(len(wordList))]
#tfidf
\end{python}

\pbitem Solution
\vspace{2ex}

The cosine similarity between the ﬁrst and the second review in terms of their tf-idf representations: 0.106130241679

\begin{center} 
Listing 6: Key code for Prob.6
\end{center}
\begin{python}
wordCount1 = defaultdict(int)
punctuation = set(string.punctuation)

for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  w = r.split()
  for word in wordList:
    if word in w:
      wordCount1[word] += 1

freq = [math.log10(len(data) * 1.0 /wordCount1[word]) for word in wordList]
d=data[0]
wordCount2 = defaultdict(int)
r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
w = r.split()
for word in w:
    if word in wordList:
      wordCount2[word] += 1
tfidf1=[freq[i]*wordCount2[wordList[i]] for i in range(len(wordList))]

d=data[1]
wordCount2 = defaultdict(int)
r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
w = r.split()
for word in w:
    if word in wordList:
      wordCount2[word] += 1
tfidf2=[freq[i]*wordCount2[wordList[i]] for i in range(len(wordList))]
1-scipy.spatial.distance.cosine(tfidf1,tfidf2)
\end{python}

\pbitem Solution
\vspace{2ex}

Which other review has the highest cosine similarity compared to the ﬁrst review (provide the beerId and proﬁleName, or the text of the review):

BeerId: 52211

User/profileName: 'Heatwave33'

Max cosine similarity: 0.31732766002633128

Number in the dataset: 4003

\begin{center} 
Listing 7: Key code for Prob.7
\end{center}
\begin{python}
cosine_m=0
num_max=-1

for t in range(1,len(data)):
    d=data[t]
    wordCount2 = defaultdict(int)
    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
    w = r.split()
    for word in w:
      if word in wordList:
        wordCount2[word] += 1
    tfidf2=[freq[i]*wordCount2[wordList[i]] for i in range(len(wordList))]
    tmp=1-scipy.spatial.distance.cosine(tfidf1,tfidf2)
    if(tmp>cosine_m):
      cosine_m=tmp
      num_max=t

print data[num_max]
\end{python}

\pbitem Solution
\vspace{2ex}

MSE: 0.27875956007772162

\begin{center} 
Listing 8: Key code for Prob.8
\end{center}
\begin{python}
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  for w in r.split():
    wordCount[w] += 1

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

wordList = [x[1] for x in counts[:1000]]

wordCount1 = defaultdict(int)
punctuation = set(string.punctuation)

for d in data:
  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])
  w = r.split()
  for word in wordList:
    if word in w:
      wordCount1[word] += 1

freq = [math.log10(len(data) * 1.0 /wordCount1[word]) for word in wordList]

wordId = dict(zip(words, range(len(words))))
wordSet = set(words)

def feature(datum):
  feat = [0]*len(words)
  wordCount2 = defaultdict(int)
  r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])
  w = r.split()
  for word in w:
    if word in wordList:
      wordCount2[word] += 1
  for i in range(len(wordList)):
    feat[i]=freq[i]*wordCount2[wordList[i]]
  feat.append(1) #offset
  return feat

X = [feature(d) for d in data]
y = [d['review/overall'] for d in data]

clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(X, y)
theta = clf.coef_
predictions = clf.predict(X)

diff = predictions-y
mse = sum([t**2 for t in diff])/len(diff)
\end{python}


\end{problemlist}
\end{document}
